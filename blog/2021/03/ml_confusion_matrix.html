<!DOCTYPE html><html lang="en">
    <head>
<meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1.0 user-scalable=no"/>
        <title>Working in Progress</title>
        <link rel="stylesheet" href="/css/main.css"/>
        <link rel="stylesheet" href="/css/zenburn.css"/>
        
        
    </head><body class="is-preload">
        <div id="wrapper">
            <div id="main">
                <div class="inner">
                    <header id="header">
                        <a href="/index.html" class="logo">
                            <strong>
                                WORKING IN PROGRESS
                            </strong> - POST
                        </a><ul class="icons">
                            <li>
                                <a href="/feed.xml" class="icon fa-rss">
                                    <span class="label">
                                        Twitter
                                    </span>
                                </a>
                            </li><li>
                                <a href="https://twitter.com/marioggar" class="icon fa-twitter">
                                    <span class="label">
                                        Twitter
                                    </span>
                                </a>
                            </li><li>
                                <a href="https://github.com/mariogarcia" class="icon fa-github">
                                    <span class="label">
                                        Github
                                    </span>
                                </a>
                            </li>
                        </ul>
                    </header><section><header class="main"><div class="metadata"><em class="fa fa-calendar-o"></em><b>2021-03-15</b></div><h1>Model Evaluation: Confusion Matrix</h1></header><yieldScaped><div id="preamble">
<div class="sectionbody">
<div class="imageblock text-center">
<div class="content">
<img src="/img/2021/03/ml_confusion_matrix/header.png" alt="ConfusionAndMatrix" width="100%">
</div>
</div>
<div class="paragraph">
<p>The machine learning workflow usually involves several steps: getting a <strong>representation of the data</strong>, looking for the most suitable <strong>features</strong>,
<strong>train the model</strong> with those features, and finally <strong>check whether the current model fits the solution</strong> we were aiming for or not.
This last step is called <strong>evaluation</strong> and tries to check whether our model fits our purposes or not.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="accuracy">Accuracy</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Most of the time I&#8217;m focusing on getting the highest score when training my machine learning models. The score I&#8217;m looking for is based on accuracy. The accuracy here is the <strong>number of predicted samples that were correctly labeled divided by the total number of samples</strong>.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="/img/54b5899f9d81d5aacad393ee14400f9b.png" alt="54b5899f9d81d5aacad393ee14400f9b.png" height="50">
</div>
<div class="title">Figure 1. accuracy formula</div>
</div>
<div class="paragraph">
<p>Is not a bad estimate to get started but <strong>is not a silver bullet</strong>. Depending on the problem, accuracy may not be the best evaluation method. For example, if we&#8217;re dealing with a health related machine learning problem, such as classifying a certain type of cancer cases, even a 90% of accuracy is not good enough. In this case we are ok by having false positives if we&#8217;re sure that we don&#8217;t miss any true positive. False positives can be validated by a doctor afterwards.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="confusion_matrix">Confusion Matrix</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In order to choose which could be the most suitable metric used for a given problem, we need to know which is the goal of the solution.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Is a health related app and we want to <strong>make sure we don&#8217;t miss any positive case</strong></p>
</li>
<li>
<p>We need to store items into boxes and we need to <strong>make sure the accuracy is close to 100%</strong>, in other words, to be accurate</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>One helpful way to visualize how the model is performing is to use a confusion matrix. A confusion matrix is <strong>a matrix were all posible outcomes of
the model are classified in different quadrants</strong>. Depending on which quadrant is most important for us to highlight as result,
<strong>we will choose the classifier that fits best the quadrants we are interested in</strong>. An evaluation matrix for a binary classification problem could look like the following:</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="/img/diag-b9576e02d6f87bbe683b9b888762b778.png" alt="Confusion matrix" width="370" height="308">
</div>
<div class="title">Confusion matrix</div>
</div>
<div class="paragraph">
<p>Lets create a theoretical example. Lets say we have a binary classification problem. We&#8217;re classifying images, and we need to classify 100 images between cats and dogs. The confusion matrix for my actual model is the following:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/img/diag-37ba68dacf2cfa390df4dfc1ea237905.png" alt="Binary classification" width="230" height="182">
</div>
<div class="title">Binary classification</div>
</div>
<div class="paragraph">
<p>If we establish that the positive class (1) is to find a dog and a negative class (0) is to find a cat, according to this confusion matrix of a 100 samples we can make certain assumptions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>20% of the time</strong> the model <strong>classifies correctly that a given animal is not a dog</strong> (True Negative - TN)</p>
</li>
<li>
<p><strong>30% of the time</strong> the model <strong>classifies incorrectly that a given animal is not a dog</strong>  (False Negative - FN)</p>
</li>
<li>
<p><strong>19% of the time</strong> the model <strong>classifies incorrectly that a given animal is a dog</strong> (False Positive - FP)</p>
</li>
<li>
<p><strong>31% of the time</strong> the model <strong>classifies correctly that a given animal is a dog</strong>  (True Positive - TP)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The evaluation matrix helps us to sort all of the samples in four different boxes and then applying different metrics depending on the goal we&#8217;re aiming, Do we want to not miss any dog even if we accept to classify a cat as a dog from time to time ? Do we want to have a 100% of accuracy classifying dogs ? How 100% of accuracy would be represented in the confusion matrix by the way ?</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/img/diag-89a77d67a4b8a56ee372c45cf6f23bd1.png" alt="Accuracy metric" width="230" height="182">
</div>
<div class="title">Accuracy metric</div>
</div>
<div class="paragraph">
<p>But as we&#8217;re mentioned so far, we may want to prioritize some other type of metric. Other metrics could be: <strong>precision, recall or specificity</strong>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="precision_recall_and_f1_score">Precision, Recall and F1 score</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="precision">Precision</h3>
<div class="paragraph">
<p>If you&#8217;d like to maximize precision, that would mean (in the current example), that everytime we say a given sample is a dog, it&#8217;s going to be a dog. But that also could mean that when we say something is a cat we may find out it&#8217;s a dog.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/img/96d2dbcd621a2da7cff58eaed68d87c2.png" alt="96d2dbcd621a2da7cff58eaed68d87c2.png" height="50">
</div>
<div class="title">Figure 2. precision formula</div>
</div>
<div class="paragraph">
<p>In our example our first model was performing like this:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/img/diag-68d1981304602339732316ef075af471.png" alt="Precision" width="230" height="182">
</div>
<div class="title">Precision</div>
</div>
<div class="paragraph">
<p>Calculating the precision would give us 31 / 31 + 19 = 0.62, 62% precision. A model that improves precision would look like the following:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/img/diag-4f5e940fa1cc0908c1d1c91a57c7e629.png" alt="Improved precission" width="230" height="182">
</div>
<div class="title">Improved precission</div>
</div>
<div class="paragraph">
<p>Which would give us a precision of: 41 / 41 + 0 = 1, 100% precision. Notice we are still missing some dogs (49 dogs are classified as false negatives), but here <strong>the point is that whenever the model say it&#8217;s a dog, it&#8217;s a dog</strong>.</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">Precision is good for&#8230;&#8203;</div>
<div class="paragraph">
<p>In general <strong>precision could be a good metric for</strong> tasks such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>search engine rankings, query suggestion</p>
</li>
<li>
<p>document classification</p>
</li>
<li>
<p>many customer-facing tasks as customers are very sensitive to errors</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="recall">Recall</h3>
<div class="paragraph">
<p>What if we don&#8217;t want to miss any dog even though we can end up classifying cats as a dog ? We want a high recall. First the formula:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/img/9fc030ab004eb3b12a815227ce62da52.png" alt="9fc030ab004eb3b12a815227ce62da52.png" height="50">
</div>
<div class="title">Figure 3. recall formula</div>
</div>
<div class="paragraph">
<p>We start with a model that performs like this:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/img/diag-5617d16bad9ec93391f16c79749a657c.png" alt="Recall" width="230" height="182">
</div>
<div class="title">Recall</div>
</div>
<div class="paragraph">
<p>Calculating the recall we end up with: 31 / 31 + 30 = 0.508, around 50% of time time when we say it&#8217;s a dog it&#8217;s a dog, that&#8217;s not better than flipping a coin. Let&#8217;s checkout another model:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/img/diag-17d45278c488a99230a68ffe7a7f7708.png" alt="Improved recall" width="230" height="182">
</div>
<div class="title">Improved recall</div>
</div>
<div class="paragraph">
<p>In this ocassion we&#8217;ve got that: 31 / 31 + 0 = 1, 100% of recall. This time this would mean that every time we say a dog is a dog we may get a cat, but we are completely sure, that no dogs have been classified as cats. Looking at the matrix:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>50% of the samples</strong> have been <strong>correctly classified as cats</strong> (True Negative - TN)</p>
</li>
<li>
<p><strong>0% of the samples</strong> have been <strong>incorrectly classified as cats</strong> (False Negative - FN)</p>
</li>
<li>
<p><strong>19% of the samples</strong> have been <strong>incorrectly classified as dogs</strong> (False Positive - FP)</p>
</li>
<li>
<p><strong>31% of the samples</strong> have been <strong>correctly classified as dogs</strong> (True Positive - TP)</p>
</li>
</ul>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">Recall is good for&#8230;&#8203;</div>
<div class="paragraph">
<p>In general <strong>recall could be a good metric for tasks</strong> such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Search and information extraction in legal discovery</p>
</li>
<li>
<p>tumor detection</p>
</li>
<li>
<p>tasks with a man-in-the-loop to filter out false positives</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="f1_score">F1 score</h3>
<div class="paragraph">
<p>F1 score combines precision and recall into a single number. The formula is:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/img/f7ce964042ecded59c4495ac23e5e10b.png" alt="f7ce964042ecded59c4495ac23e5e10b.png" height="50">
</div>
<div class="title">Figure 4. f1 score formula</div>
</div>
<div class="paragraph">
<p>As it&#8217;s mentioned <a href="https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9">this article</a> F1 Score might be a better measure to use <strong>if we need to seek a balance between Precision and Recall AND there is an uneven class distribution</strong> (large number of Actual Negatives).</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="using_scikit_to_show_metrics">Using Scikit to show metrics</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To show how confusion matrix can help deciding which classifier to use I&#8217;m using a dataset from Madrid City Open Data.
This <a href="https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=50d7d35982d6f510VgnVCM1000001d4a900aRCRD&amp;vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&amp;vgnextfmt=default">dataset</a>
shows the interventions of the urgency and rescue city service (SAMUR in spanish) in 2020.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="/img/2021/03/ml_confusion_matrix/samur_data.png" alt="SAMUR" width="100%">
</div>
</div>
<div class="paragraph">
<p>With this dataset I&#8217;d like to be able to find a classifier that answers the question: <strong>Is this type of intervention going to take
to the urgency service more time than average to respond ?</strong></p>
</div>
<div class="paragraph">
<p>First I did some work to prepare the data the best I could:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Converting categorical data to numerical</p>
</li>
<li>
<p>Applying under-sampling to correct imbalance</p>
</li>
<li>
<p>Choosing best features</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Then I chose several classifiers:</p>
</div>
<div class="listingblock">
<div class="title">classifiers</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.dummy import DummyClassifier

# different classifiers
logistic    = LogisticRegression().fit(X_train, y_train)
svc         = LinearSVC(random_state=0).fit(X_train, y_train)
knn         = KNeighborsClassifier(5).fit(X_train, y_train)
tree        = DecisionTreeClassifier(max_depth=3).fit(X_train, y_train)
rff         = RandomForestClassifier(max_depth=2, n_estimators=4, max_features=2).fit(X_train, y_train)
ada         = AdaBoostClassifier(n_estimators=100).fit(X_train, y_train)
dummy       = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)</code></pre>
</div>
</div>
<div class="paragraph">
<p>And then I printed out their confusion matrices:</p>
</div>
<div class="listingblock">
<div class="title">confusion matrix of every classifier</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">from sklearn.metrics import confusion_matrix

classifiers = [dummy, logistic, svc, knn, tree, rff, ada]

for clsf in classifiers:
    y_predicted = clsf.predict(X_test)
    print(clsf.__class__.__name__)
    print('===========================================')
    print(confusion_matrix(y_test, y_predicted))
    print()</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here&#8217;s a nicer representation of the confusion matrices.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/img/diag-83c2392ea6863c27796bbacd74ed2018.png" alt="Confusion matrices" width="860" height="350">
</div>
<div class="title">Confusion matrices</div>
</div>
<div class="paragraph">
<p>First impressions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>None of them</strong> is performing at a decent level</p>
</li>
<li>
<p>Is <strong>not even possible to see a decent recall</strong> because both dummy and linear classifiers are the different sides of choosing the most frequent outcome&#8230;&#8203; useless.</p>
</li>
<li>
<p>I&#8217;d say that <strong>the classifier  that gives me the highest precision is the AdaBoostClassifier</strong>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>But to be sure, I&#8217;m producing the precision, recall and f1-score summary with sklearn&#8217;s <strong>classification_report</strong> of each of them:</p>
</div>
<div class="listingblock">
<div class="title">summary</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">from sklearn.metrics import classification_report

classifiers = [dummy, logistic, svc, knn, tree, rff, ada]

for clsf in classifiers:
    y_predicted = clsf.predict(X_test)
    print(clsf.__class__.__name__)
    print('===========================================')
    print(classification_report(y_test, y_predicted, target_names = ['above_avg', 'not_above_avg']))</code></pre>
</div>
</div>
<div class="paragraph">
<p>I&#8217;m summarizing the results in the following table:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Highest results for possitive class</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Classifier</th>
<th class="tableblock halign-left valign-top">Precision</th>
<th class="tableblock halign-left valign-top">Recall</th>
<th class="tableblock halign-left valign-top">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DummyClassifier</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.50</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.00</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.57</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">LogisticRegression</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.57</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.70</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.63</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">LinearSVC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.00</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.00</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.00</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">KNeighborsClassifier</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.61</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.61</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.61</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DecisionTreeClassifier</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.56</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>0.83</strong> &#8592;----</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.67</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">RandomForestClassifier</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.58</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.66</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.61</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">AdaBoostClassifier</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>0.62</strong> &#8592;----</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.70</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.66</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The summary is telling me that the classifier that ranks best for <strong>precision</strong> is the <strong>AdaBoostClassifier</strong> and the classifier that
ranks best for <strong>recall</strong> is the <strong>DecisionTreeClassifier</strong>. The take-away is:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If I wanted to be sure that <strong>I&#8217;m missing the few positive samples possible</strong> I&#8217;d look for the <strong>highest recall</strong> classifier</p>
</li>
<li>
<p>If I wanted to be sure that <strong>I&#8217;m getting a true positive sample every time</strong> I&#8217;d look for the <strong>highest precision</strong> classifier</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>And finally, <strong>this whole evaluation process has serve to realize that I need to go back to square one</strong>, because as I mentioned
earlier <strong>NONE of the classifiers is performing decently</strong>, so I need to <strong>check where I can improve the process</strong>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Enrich data</p>
</li>
<li>
<p>Increase/Decrease samples</p>
</li>
<li>
<p>Tune classifiers to get the desired precision/recall/accuracy&#8230;&#8203;</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="resources">Resources</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</a>: Wikipedia definition</p>
</li>
<li>
<p><a href="https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234">Different Evaluation Metrics</a>: Interesting article talking about different evaluation metrics</p>
</li>
<li>
<p><a href="https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9">Accuracy, precision, recall or F1</a>: article in towardsdatascience.com</p>
</li>
<li>
<p><a href="/files/2021/03/ml_confusion_matrix/SAMUR.ipynb">SAMUR.ipynb Jupyter Notebook</a></p>
</li>
</ul>
</div>
</div>
</div></yieldScaped></section>
                </div>
            </div><div id="sidebar">
                <div class="inner">
                    <!--Menu--><nav id="menu">
                        <header class="major">
                            <h2>Menu</h2>
                        </header><ul>
                            <li>
                                <a href="/index.html">Latests entries</a>
                            </li><li>
                                <a href="/archive.html">Archive</a>
                            </li><li>
                                <a href="/stats.html">Statistics</a>
                            </li><li>
                                <a href="/about.html">About</a>
                            </li>
                        </ul>
                    </nav>
                </div>
            </div><script src="/js/jquery.min.js"></script>
            <script src="/js/browser.min.js"></script>
            <script src="/js/breakpoints.min.js"></script>
            <script src="/js/util.js"></script>
            <script src="/js/main.js"></script>
            <script src="/js/highlight.pack.js"></script>
            <script type="text/javascript">
                
        document.addEventListener('DOMContentLoaded', (event) => {
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightBlock(block);
            });
        });
    
            </script>
        </div>
    </body>
</html>
