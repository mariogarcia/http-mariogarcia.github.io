<!DOCTYPE html><html lang="en">
    <head>
<meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1.0 user-scalable=no"/>
        <title>Working in Progress</title>
        <link rel="stylesheet" href="/css/main.css"/>
        <link rel="stylesheet" href="/css/zenburn.css"/>
        
        
    </head><body class="is-preload">
        <div id="wrapper">
            <div id="main">
                <div class="inner">
                    <header id="header">
                        <a href="/index.html" class="logo">
                            <strong>
                                WORKING IN PROGRESS
                            </strong> - POST
                        </a><ul class="icons">
                            <li>
                                <a href="/feed.xml" class="icon fa-rss">
                                    <span class="label">
                                        Twitter
                                    </span>
                                </a>
                            </li><li>
                                <a href="https://twitter.com/marioggar" class="icon fa-twitter">
                                    <span class="label">
                                        Twitter
                                    </span>
                                </a>
                            </li><li>
                                <a href="https://github.com/mariogarcia" class="icon fa-github">
                                    <span class="label">
                                        Github
                                    </span>
                                </a>
                            </li>
                        </ul>
                    </header><section><header class="main"><div class="metadata"><em class="fa fa-calendar-o"></em><b>2021-04-18</b></div><h1>NLP: Text Similarity</h1></header><yieldScaped><div id="preamble">
<div class="sectionbody">
<div class="imageblock text-center">
<div class="content">
<img src="/img/2021/04/nlp/nlp_header.png" alt="nlp" width="100%">
</div>
</div>
<div class="paragraph">
<p>In NLP, the study of text similarities could be very helpul for tasks such as <strong>looking for similar texts</strong>, or creating <strong>spell checkers</strong> replacing a mispelled word by the right one.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="looking_for_similar_texts">Looking for similar texts</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>In order to measure how similar two different texts are, we usually calculate "the distance" between them</strong>, how far two text  are to be the same. There are a few <a href="https://www.nltk.org/_modules/nltk/metrics/distance.html">distance metrics available in NLTK</a>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Jaccard distance</p>
</li>
<li>
<p>Levenshtein edit-distance</p>
</li>
<li>
<p>Jaro-Winkler distance</p>
</li>
<li>
<p>&#8230;&#8203;</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In this article I&#8217;ll be using the Jaccard distance to calculate the similarity of two texts. I leave some links about the theory behind the Jaccard distance in the resources section.</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">SIMILARITY != DISTANCE</div>
<div class="paragraph">
<p>Similarity is not the same as distance. While similarity is how similar a text is compared to another one, distance would be how far is a given text to be the same as another text. They&#8217;re kind two sides of the same story. Mathematically speaking The similarity is 1 minus the distance between both texts, therefore, regarding Jaccard distance / similarity:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>a <strong>similarity of 1</strong> means both texts are <strong>identical</strong></p>
</li>
<li>
<p>a <strong>similarity of 0</strong> means both texts have <strong>nothing in common</strong></p>
</li>
<li>
<p>a <strong>distance of 0</strong> means both texts are <strong>identical</strong></p>
</li>
<li>
<p>a <strong>distance of 1</strong> means both texts have <strong>nothing in common</strong></p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>I went to a famous sentences site and I picked a few and mixed them with another few made-up sentences:</p>
</div>
<div class="listingblock">
<div class="title">famous sentences</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">sentences = [
"If you look at what you have in life, you'll always have more. If you look at what you don't have in life, you'll never have enough.",
"Are you ok with that situation ?",
"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.",
"Life is what happens when you're busy making other plans.",
"I don't like to wake up early",
"Did you do your homework ?",
"This sentences is made up to be an outsider",
"I don't like the way he drives"
]</code></pre>
</div>
</div>
<div class="paragraph">
<p>So I&#8217;m basically going <strong>to compare all of them using the Jaccard similarity</strong>, which, as I mentioned earlier would be the result of 1 minus the Jaccard distance between two sentences. I&#8217;m calculating Jaccard similarity using <a href="https://www.nltk.org/_modules/nltk/metrics/distance.html">NLTK&#8217;s nltk.metrics.distance.jaccard_distance</a> in the following function:</p>
</div>
<div class="listingblock">
<div class="title">jaccard similarity with NLTK</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">from nltk.metrics.distance import jaccard_distance

def jaccard_similarity(x, y):
    return 1 - jaccard_distance(x, y)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <strong>jaccard_distance</strong> function receives two sequences (x and y). These sequences could be:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>sequences of characters</p>
</li>
<li>
<p>sequences of tokens</p>
</li>
<li>
<p>sequences of n-grams</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>We&#8217;ll see along the next sections how <strong>using one type of sequences or another will impact in the distance result</strong>.</p>
</div>
<div class="sect2">
<h3 id="comparing_characters">Comparing characters</h3>
<div class="paragraph">
<p>The first stop it&#8217;s to compare texts by their characters. Our initial asumption is that a given text could be similar to another text if they share enough characters between them. With that in mind we&#8217;re calculating the Jaccard similarity between all sentences, this time <strong>using sequences of characters</strong>.</p>
</div>
<div class="listingblock">
<div class="title">creating an 8x8 matrix with Jaccard similarities</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import pandas as pd

rows = []

for text_1 in sentences:
    row = []
    for text_2 in sentences:
        sequence_1 = set(text_1) # sequence of characters
        sequence_2 = set(text_2) # sequence of characters

        row.append(jaccard_similarity(sequence_1, sequence_2))
    rows.append(row)

df = pd.DataFrame(data=rows, columns=["sentence_{}".format(i + 1) for i in range(0,8)])</code></pre>
</div>
</div>
<div class="paragraph">
<p>Remember that <strong>the arguments for the jaccard_distance function are sequences</strong>. In this particular case I&#8217;m passing a sequence of characters as a result of invoking <strong>set(string)</strong> which returns all unique characters contained in the string passed as parameter. As I&#8217;ll point out in the next section, you can also pass a sequence of words (tokens) or n-grams.</p>
</div>
<div class="paragraph">
<p>I&#8217;m using a <strong>heatmap to highlight which sentences are more similar than others</strong>. The darker the color the more similar two sentences are. You can see how the main diagonal is comparing a sentence with itself, that&#8217;s why it&#8217;s always 1.</p>
</div>
<div class="listingblock">
<div class="title">similarity matrix</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

corr_matrix = np.corrcoef(df.T)

plt.figure(figsize=(5, 5))
sns.heatmap(
    corr_matrix,
    cbar=False,
    annot=True,
    square=True,
    linewidths=.5,
    cmap="YlGnBu",
    xticklabels=df.columns,
    yticklabels=df.columns)</code></pre>
</div>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="/img/2021/04/nlp/jaccard_similarities_heatmap.png" alt="jaccard similarities" width="40%">
</div>
</div>
<div class="paragraph">
<p>After looking the heatmap carefully it seems that sentences 5 and 8 have a higher similarity than the rest:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">"I don't like to wake up early"
"I don't like the way he drives"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Well, both sentences seemed to have similar grammar constructs, yes. But <strong>there&#8217;re other pairs that have a relative high similarity even though it&#8217;s clear that they don&#8217;t have much to do with each other</strong>. There must be a better way of comparing texts without that much noise.</p>
</div>
</div>
<div class="sect2">
<h3 id="comparing_tokens">Comparing tokens</h3>
<div class="paragraph">
<p>As a perfect example of how wrong could be to compare texts using just characters, you can find the following sentences:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">from nltk.metrics.distance import jaccard_distance

sentence_1 = "aloha amigo is fun"
sentence_2 = "I am a go fan"

similarity_by_chars = 1 - jaccard_distance(set(sentence_1), set(sentence_2))
similarity_by_chars</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">0.5384615384615384</code></pre>
</div>
</div>
<div class="paragraph">
<p>As I was saying, altough they have nothing to do with each other, still the output raised a 0.53 similarity which makes little sense. On the other hand if we compare both sentences using a sequence of tokens we will demostrate that they have nothing to do with each other:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import nltk
from nltk.metrics.distance import jaccard_distance

def get_similarity_by_tokens(left, right):
    tokens_1 = set(nltk.word_tokenize(left)) # sequence of tokens
    tokens_2 = set(nltk.word_tokenize(right)) # sequence of tokens

    similarity = 1 - jaccard_distance(tokens_1, tokens_2)
    return similarity


similarity_by_tokens = get_similarity_by_tokens(sentence_1, sentence_2)
similarity_by_tokens</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">0.0</code></pre>
</div>
</div>
<div class="paragraph">
<p>However comparing two similar sentences outputs a fair result:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">sentence_1 = "I am a big fan"
sentence_2 = "I am a tennis fan"

similarity_by_tokens = get_similarity_by_tokens(sentence_1, sentence_2)
similarity_by_tokens</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">0.6666666666666667</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="comparing_n_grams">Comparing n-grams</h3>
<div class="paragraph">
<p><strong>A n-gram is a sequence of phonems where n could be any positive number</strong>. That allows our comparison procedure to be smarter. Instead of comparing all characters, or word by word, in this case, we would like to find certain tuples of common tokens between two sentences. For example, <strong>What are the n-grams of sentence_1, with n=2 using tokens ?</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import nltk
from nltk.util import ngrams

seq_1 = set(nltk.word_tokenize("I am a big fan"))
seq_2 = set(nltk.word_tokenize("I am a tennis fan"))

list(ngrams(seq_1, n=2)), list(ngrams(seq_2, n=2))</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">n-grams</div>
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">([('am', 'fan'), ('fan', 'big'), ('big', 'I'), ('I', 'a')],
 [('am', 'tennis'), ('tennis', 'fan'), ('fan', 'I'), ('I', 'a')])</code></pre>
</div>
</div>
<div class="paragraph">
<p>Which shows that only 1 out of 4 tuples is the same. Now I&#8217;m creating a function which calculates the similarity using n-grams and jaccard distance:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">def get_similarity_by_ngrams(left, right, n):
    seq_1 = set(ngrams(nltk.word_tokenize(left), n=n))
    seq_2 = set(ngrams(nltk.word_tokenize(right), n=n))

    similarity = 1 - jaccard_distance(seq_1, seq_2)
    return similarity</code></pre>
</div>
</div>
<div class="paragraph">
<p>And apply it to the previous sentences:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">sentence_1 = "I am a big fan"
sentence_2 = "I am a tennis fan"

similarity_by_tokens = get_similarity_by_ngrams(sentence_1, sentence_2, n=2)
similarity_by_tokens</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">0.33333333333333337</code></pre>
</div>
</div>
<div class="paragraph">
<p>Which outputs something I would expect.</p>
</div>
<div class="paragraph">
<p>In this section, we&#8217;ve tried different strategies of comparing texts, we started comparing characters, then moving to tokens, and finally with n-grams. The overall idea, is that every step we took the higher was the normalization of the comparison. <strong>With characters we had a lot of noise</strong> trying to find similar sentences and as we went through the other to types, we managed to reduce the noise by a lot. In the end <strong>n-grams and specially the n parameter could reduce the noise as a form of normalization</strong>.</p>
</div>
<div class="quoteblock">
<blockquote>
n-grams and specially the n parameter could be used as a form of normalization
</blockquote>
</div>
<div class="paragraph">
<p>Before moving forward, lets show the heatmap of the beginning using n-grams (n=2):</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="/img/2021/04/nlp/jaccard_similarities_heatmap_with_ngrams.png" alt="jaccard similarities" width="40%">
</div>
</div>
<div class="paragraph">
<p><strong>Now it&#8217;s pretty clear</strong> that the only two sentences with some significant similarity are the 5th and 8th:</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="a_simple_spell_checker">A simple spell checker</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Another possible use of text similarity is to correct spelling mistakes. For instance, lets say we&#8217;ve got the following text written incorrectly:</p>
</div>
<div class="quoteblock">
<blockquote>
it could be a greal busines
</blockquote>
</div>
<div class="paragraph">
<p>For us, humans, is easy to check the dictionary and spot the error we did while writing a word, but it&#8217;s not as easy for a computer. In this section I&#8217;m using NLTK and the Jaccard distance to create a basic spell checker. What do we need to create a spell checker ?</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>A corpus</strong>: the ground truth where the correct words can be found. Generally speaking a <strong>corpus would be the dictionary where we go looking for the right spelling</strong></p>
</li>
<li>
<p><strong>A method</strong>: to check the similarity between the wrong word and the most similar word from the corpus. The method in this case would be the Jaccard similarity.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>So in this context <strong>we&#8217;re using n-grams to establish which is the minimal number of similar phonemes to compare</strong>. In other words, instead of comparing two words letter by letter, we will be comparing two words by n-grams.</p>
</div>
<div class="listingblock">
<div class="title">using jaccard distance</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">from nltk.util import ngrams

def get_ngrams(word, n):
    return set(ngrams(word,n=n))

def get_recommendation_for(word, n=3):
    from nltk.corpus import words
    from nltk.metrics.distance import jaccard_distance

    # we don't want stop-words to be processed
    if len(word) &lt; 3:
        return word

    ngram        = get_ngrams(word, n)
    spellings    = words.words()
    scores       = [(w, jaccard_distance(ngram, get_ngrams(w, n))) for w in spellings if w.startswith(word[0])]
    higest_first = sorted(scores, key=lambda entry: entry[1])

    return highest_first[0][0]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we can apply the recommender with our initial text using <strong>n=3</strong>:</p>
</div>
<div class="listingblock">
<div class="title">applying jaccard distance recommender</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">wrong_text     = "it could be a greal busines"
tokens         = nltk.word_tokenize(wrong_text)
correct_words  = [get_recommendation_for(w) for w in tokens]
corrected_text = " ".join(correct_words)

print(corrected_text)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Which outputs:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>it could be a great business</pre>
</div>
</div>
<div class="paragraph">
<p>I&#8217;m curious and I wanted to see what the output would be <strong>playing with the value of n</strong>, and for <strong>n=1</strong> (aka comparing letter by letter) the spell checker turned out to be a spell mess:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>it cloud be a gaggler busine</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="resources">Resources</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard Index in Wikipedia</a></p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance">Levenshtein_distance in Wikipedia</a></p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/N-gram">N-Gram in Wikipedia</a></p>
</li>
<li>
<p><a href="https://towardsdatascience.com/from-dataframe-to-n-grams-e34e29df3460">N-Gram ranking explained</a>: A quick-start guide to creating and visualizing n-gram ranking using nltk for natural language processing</p>
</li>
<li>
<p><a href="http://billchambers.me/tutorials/2014/12/21/tf-idf-explained-in-python.html" class="bare">http://billchambers.me/tutorials/2014/12/21/tf-idf-explained-in-python.html</a></p>
</li>
<li>
<p><a href="https://studymachinelearning.com/jaccard-similarity-text-similarity-metric-in-nlp/">Jaccard Similarity as similarity metric</a></p>
</li>
<li>
<p><a href="https://datascience.stackexchange.com/questions/63325/cosine-similarity-vs-the-levenshtein-distance">Cosine similarity vs The Levenshtein distance</a>: from datascience.stackexchange.com</p>
</li>
</ul>
</div>
</div>
</div></yieldScaped></section>
                </div>
            </div><div id="sidebar">
                <div class="inner">
                    <!--Menu--><nav id="menu">
                        <header class="major">
                            <h2>Menu</h2>
                        </header><ul>
                            <li>
                                <a href="/index.html">Latests entries</a>
                            </li><li>
                                <a href="/archive.html">Archive</a>
                            </li><li>
                                <a href="/stats.html">Statistics</a>
                            </li><li>
                                <a href="/about.html">About</a>
                            </li>
                        </ul>
                    </nav>
                </div>
            </div><script src="/js/jquery.min.js"></script>
            <script src="/js/browser.min.js"></script>
            <script src="/js/breakpoints.min.js"></script>
            <script src="/js/util.js"></script>
            <script src="/js/main.js"></script>
            <script src="/js/highlight.pack.js"></script>
            <script type="text/javascript">
                
        document.addEventListener('DOMContentLoaded', (event) => {
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightBlock(block);
            });
        });
    
            </script>
        </div>
    </body>
</html>
