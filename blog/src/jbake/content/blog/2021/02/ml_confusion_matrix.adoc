= Model Evaluation: Confusion Matrix
@marioggar
2021-02-22
:jbake-type: post
:jbake-status: draft
:jbake-tags: ds, ml, python
:sources: ../../../../../../../sources/2020/11/ml_key_concepts
:idprefix:
:summary: An intro on how to evaluate a supervised model
:summary_image: machine_learning.png

image::2021/02/ml_confusion_matrix/header.png[alt="ConfusionAndMatrix", align="center", width="100%"]

The machine learning workflow usually involves several steps: getting a **representation of the data**, looking for the most suitable **features**, **train the model** with those features, and finally **check whether the current model fits the solution** we were aiming for or it doesn't. This last step is called **evaluation** and tries to check whether our model fits our purposes or not.

== Accuracy

Most of the time I'm focusing on getting the highest score when training my machine learning models. The score I'm looking for is based on accuracy. The accuracy here is the **number of predicted samples that were correctly labeled divided by the total number of samples**. 

[mathx, height=50, align=center]
.accuracy formula
----
accuracy = \frac{correctly\ predicted\ samples}{total\ number\ of\ samples} 
----

Is not a bad estimate to get started but **is not a silver bullet**. Depending on the problem, accuracy may not be the best evaluation method. For example, if we're dealing with a health related machine learning problem, such as classifying a certain type of cancer cases, even a 90% of accuracy is not good enough. In this case we are ok by having false positives if we're sure that we don't miss any true positive. False positives can be validated by a doctor afterwards. 

== Confusion Matrix

In orther to choose which could be the most suitable metric used for a given problem, we need to know which is the goal of the solution. 

- Is a health related app and we want to make sure we don't miss any positive case
- We need to store items into boxes and we need to make sure the accuracy is close to 100%, in other words, to be accurate

One helpful way to visualize how the model is performing is to use a confusion matrix. A confusion matrix is a matrix were all posible outcomes of the model are classified in different quadrants. Depending on which quadrant is most important for us to highlight as result, we will choose one metric evaluation or another. An evaluation matrix for a binary classification problem could look like the following:

[ditaa, align="center"]
.Confusion matrix
....

+---------------+---------------+
|cGRE           |cPNK           |
|               |               |
|      True     |     False     |
|    Negative   |    Positive   |
|      (TN)     |      (FP)     |
|               |               |
|               |               |
+---------------+---------------+
|cPNK           |cGRE           |
|               |               |
|      False    |     True      |
|    Negative   |    Positive   |
|      (FN)     |      (TP)     |
|               |               |
|               |               |
+---------------+---------------+

....

Lets create a theoretical example. Lets say we have a binary classification problem. We're classifying images, and we need to classify 100 images between cats and dogs. The confusion matrix for my actual model is the following:

[ditaa]
.Binary classification
....
+--------+--------+
|        |        |
|   20   |   19   |
|        |        |
+--------+--------+
|        |        |
|   30   |   31   |
|        |        |
+--------+--------+
....

If we establish that the positive class (1) is to find a dog and a negative class (0) is to find a cat, according to this confusion matrix of a 100 samples we can make certain assumptions:

- **20% of the time** the model **classifies correctly that a given animal is not a dog** (True Negative - TN)
- **30% of the time** the model **classifies incorrectly that a given animal is not a dog**  (False Negative - FN)
- **19% of the time** the model **classifies incorrectly that a given animal is a dog** (False Positive - FP)
- **31% of the time** the model **classifies correctly that a given animal is a dog**  (True Positive - TP)

The evaluation matrix helps us to sort all of the samples in four different boxes and then applying different metrics depending on the goal we're aiming, Do we want to not miss any dog even if we accept to classify a cat as a dog from time to time ? Do we want to have a 100% of accuracy classifying dogs ? How 100% of accuracy would be represented in the confusion matrix by the way ? 

[ditaa]
.Accuracy metric
....
+--------+--------+
|        |        |
|   50   |    0   |
|        |        |
+--------+--------+
|        |        |
|    0   |   50   |
|        |        |
+--------+--------+
....


But as we're mentioned so far, we may want to prioritize some other type of metric. Other metrics could be: **precision, recall or specificity**.

== Precision, Recall and F1 score

=== Precision

If you'd like to maximize precision, that would mean (in the current example), that everytime we say a given sample is a dog, it's going to be a dog. But that also could mean that when we say something is a cat we may find out it's a dog.

[mathx, height=50]
.precision formula
----
precision = \frac{TP}{TP + FP} 
----

In our example our first model was performing like this:

[ditaa]
.Precision
....
+--------+--------+
|        |        |
|   20   |   19   |
|        |        |
+--------+--------+
|        |        |
|   30   |   31   |
|        |        |
+--------+--------+
....

Calculating the precision would give us 31 / 31 + 19 = 0.62, 62% precision. A model that improves precision would look like the following:

[ditaa]
.Improved precission
....
+--------+--------+
|        |        |
|   10   |    0   |
|        |        |
+--------+--------+
|        |        |
|   49   |   41   |
|        |        |
+--------+--------+
....

Which would give us a precision of: 41 / 41 + 0 = 1, 100% precision. Notice we are still missing some dogs (49 dogs are classified as false negatives), but here **the point is that whenever the model say it's a dog, it's a dog**. 

.Precision is good for...
****
In general **precision could be a good metric for** tasks such as:

- search engine rankings, query suggestion
- document classification
- many customer-facing tasks as customers are very sensitive to errors
****

=== Recall

What if we don't want to miss any dog even though we can end up classifying cats as a dog ? We want a high recall. First the formula:

[mathx, height=50]
.recall formula
----
recall = \frac{TP}{TP + FN} 
----

We start with a model that performs like this:

[ditaa]
.Recall
....
+--------+--------+
|        |        |
|   20   |   19   |
|        |        |
+--------+--------+
|        |        |
|   30   |   31   |
|        |        |
+--------+--------+
....

Calculating the recall we end up with: 31 / 31 + 30 = 0.508, around 50% of time time when we say it's a dog it's a dog, that's not better than flipping a coin. Let's checkout another model:

[ditaa]
.Improved recall
....
+--------+--------+
|        |        |
|   50   |   19   |
|        |        |
+--------+--------+
|        |        |
|    0   |   31   |
|        |        |
+--------+--------+
....

In this ocassion we've got that: 31 / 31 + 0 = 1, 100% of recall. This time this would mean that every time we say a dog is a dog we may get a cat, but we are completely sure, that no dogs have been classified as cats. Looking at the matrix:

- **50% of the samples** have been **correctly classified as cats** (True Negative - TN)
- **0% of the samples** have been **incorrectly classified as cats** (False Negative - FN)
- **19% of the samples** have been **incorrectly classified as dogs** (False Positive - FP)
- **31% of the samples** have been **correctly classified as dogs** (True Positive - TP)

.Recall is good for...
****
In general **recall could be a good metric for tasks** such as:

- Search and information extraction in legal discovery
- tumor detection
- tasks with a man-in-the-loop to filter out false positives
****

=== F1 score

F1 score combines precision and recall into a single number. The formula is:

[mathx, height=50]
.f1 score formula
----
F1 = 2 * \frac{PRECISION * RECALL}{PRECISION + RECALL} = 2 * \frac{2 * TP}{2 * TP + FN + FP}
----

As it's mentioned https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9[this article] F1 Score might be a better measure to use **if we need to seek a balance between Precision and Recall AND there is an uneven class distribution** (large number of Actual Negatives).

== Using Scikit to show metrics

TODO

== Resources

- https://en.wikipedia.org/wiki/Confusion_matrix[Confusion Matrix]: Wikipedia definition
- https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234[Different Evaluation Metrics]: Interesting article talking about different evaluation metrics
- https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9[Accuracy, precision, recall or F1]: article in towardsdatascience.com